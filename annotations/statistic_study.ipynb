{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac66d3a4-eaa7-4b87-868e-ac1190a61992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import sklearn.cluster as clust\n",
    "import sklearn.metrics as met\n",
    "import numpy as np\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "import statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bed167-ee31-43cd-a650-f2d6afa77b43",
   "metadata": {},
   "source": [
    "# Cohen's kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d9f25-bfd8-43c1-8345-de676ed9ee28",
   "metadata": {},
   "source": [
    "Cohen's kappa is a statistic used to measure the inter-rater reliability between 2 raters\n",
    "It is calculated this way : \\\n",
    "Suppose that we ask two annotators, A and B, to label a sample with YES or NO, and we get the following result\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    " & YES (A) & NO (A) \\\\\n",
    " \\hline\n",
    " YES (B) & a & b \\\\\n",
    " \\hline\n",
    " NO (B) & c & d  \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "Let $ p_0 $ be the observed proportionate agreement, defined as :\n",
    "$$ p_0 = \\frac{a+d}{a+b+c+d} $$\n",
    "The estimated propability that A and B both say YES is :\n",
    "$$ p_{YES} = \\frac{a+b}{a+b+c+d}.\\frac{a+c}{a+b+c+d} $$\n",
    "The estimated propability that A and B both say NO is :\n",
    "$$ p_{NO} = \\frac{b+d}{a+b+c+d}.\\frac{c+d}{a+b+c+d} $$\n",
    "The estimated overall propability that A and B agree is then :\n",
    "$$  p_e = p_{YES} + p_{NO} $$\n",
    "Cohen's Kappa is the computed this way :\n",
    "$$ \\kappa = \\frac{p_0 - p_e}{1 - p_e} $$ \n",
    "Given the value of kappa, it is possible to evaluate the agreement between the two annotators\n",
    "$$\n",
    "\\begin{array}{|c|c|}\n",
    "\\hline\n",
    "\\kappa & \\text{\\textbf{agreement}} \\\\\n",
    " \\hline\n",
    "< 0 & \\text{poor agreement} \\\\\n",
    " \\hline\n",
    "0.01 - 0.2 & \\text{slight agreement} \\\\\n",
    " \\hline\n",
    " 0.21 - 40 & \\text{fair agreement} \\\\\n",
    " \\hline\n",
    " 0.41 - 0.6 & \\text{moderate agreement} \\\\\n",
    " \\hline\n",
    " 0.61 - 0.8 & \\text{substential agreement} \\\\\n",
    " \\hline\n",
    " 0.81 - 1.0 & \\text{almost perfect agreement} \\\\\n",
    " \\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6274ad9d-8a88-468b-983c-e9466c3b08e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations de A : [0 0 1 ... 0 0 1]\n",
      "Annotations de B : [1 1 1 ... 0 0 1]\n",
      "Score : 0.016928855261009068\n"
     ]
    }
   ],
   "source": [
    "constant_size = 50 # size of the part that is identical in the two samples\n",
    "random_size = 1000 # size of the part that may differ in the two samples \n",
    "constant = np.random.randint(2, size=constant_size) \n",
    "annot_A = np.concatenate( [np.random.randint(2, size=random_size), constant] )\n",
    "annot_B = np.concatenate( [np.random.randint(2, size=random_size), constant] )\n",
    "print(f\"Annotations de A : {annot_A}\")\n",
    "print(f\"Annotations de B : {annot_B}\")\n",
    "score = met.cohen_kappa_score(annot_A, annot_B)\n",
    "print(f\"Score : {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a652d59-b8d5-4936-b94c-f09346b81e8f",
   "metadata": {},
   "source": [
    "# Fleiss' kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5f1fb-d3d5-4558-8deb-2ad93e8cbfe4",
   "metadata": {},
   "source": [
    "Fleiss's kappa is similar to Cohen's kappa, but used to measure the agreement in a group of more than 2 annotators \\\n",
    "We consider $n$ annotated tweets (indexed by $i = 1,\\dots, n$), and k annotators (indexed by $j = 1,\\dots,k$) \\\n",
    "Let $Y_i$ be the number of annotators that labelled YES for the $i^{th}$ tweet, and $N_i$ be the number of annotators that labelled NO for the $i^{th}$ tweet \\\n",
    "Let $p_{YES}$ be the proportion of YES ratings :\n",
    "$$ p_{YES} = \\frac{1}{n.k}\\sum_{i=1}^n Y_i$$\n",
    "Let $p_{NO}$ be the proportion of NO ratings :\n",
    "$$ p_{NO} = \\frac{1}{n.k}\\sum_{i=1}^n N_i$$\n",
    "And let $\\overline{P}_e$ be :\n",
    "$$ \\overline{P}_e = (p_{YES})^2 + (p_{NO})^2$$\n",
    "Let $P_i$ be an estimation of the agreement between all the annotators on the $i^{th}$ tweet :\n",
    "$$ P_i = \\frac{Y_i(Y_i -1) + N_i(N_i - 1)}{k(k-1)}  $$ \n",
    "And $\\overline{P}$ be the mean of all the $\\overline{P}_i$ :\n",
    "$$ \\overline{P} = \\frac{1}{n}.\\sum_{i=1}^n P_i $$\n",
    "We compute Fleiss' kappa using the following formula :\n",
    "$$ \\kappa = \\frac{\\overline{P} - \\overline{P}_e}{1 - \\overline{P}_e} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ba1c75-0bc8-41a0-b580-c1030904cc38",
   "metadata": {},
   "source": [
    "Let's assume we have the following results :\n",
    "$$ \n",
    "\\begin{array}{|c|ccc|}\n",
    "\\hline\n",
    "& \\text{annotator A} & \\text{annotator B} & \\text{annotator C} \\\\\n",
    "\\hline\n",
    "\\text{tweet } 1 & 1 & 0 & 1 \\\\\n",
    "\\text{tweet } 2 & 1 & 1 & 0 \\\\\n",
    "\\text{tweet } 3 & 1 & 1 & 0 \\\\\n",
    "\\text{tweet } 4 & 1 & 1 & 1 \\\\\n",
    "\\text{tweet } 5 & 0 & 0 & 0 \\\\\n",
    "\\text{tweet } 6 & 0 & 1 & 0 \\\\\n",
    "\\text{tweet } 7 & 0 & 0 & 0 \\\\\n",
    "\\text{tweet } 8 & 1 & 1 & 1 \\\\\n",
    "\\text{tweet } 9 & 1 & 1 & 1 \\\\\n",
    "\\text{tweet } 10 & 1 & 1 & 1 \\\\\n",
    "\\text{tweet } 11 & 1 & 0 & 0 \\\\\n",
    "\\text{tweet } 12 & 0 & 0 & 0 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9dd6f25-90a1-4869-9a0c-65ef67d056b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21271/3994760343.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfleiss_kappa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minter_rater\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate_raters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fleiss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Score : {score}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/statsmodels/stats/inter_rater.py\u001b[0m in \u001b[0;36mfleiss_kappa\u001b[0;34m(table, method)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \"\"\"\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#avoid integer division\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m     \u001b[0mn_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cat\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0mn_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "data = [[1,0,1], [1,1,0], [1,1,0], [1,1,1], [0,0,0], [0,1,0], [0,0,0], [1,1,1], [1,1,1], [1,1,1], [1,0,0], [0,0,0]]\n",
    "score = sm.stats.fleiss_kappa(statsmodels.stats.inter_rater.aggregate_raters(data), method='fleiss')\n",
    "print(f\"Score : {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f10c9d6-f5be-43ac-8a87-3ecadd2c4b09",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8966d12e-e31b-437e-9595-d1429663c28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = np.array( [[1,1,0], [1,1,1], [0,1,1], [1,1,0], [0,1,0], [0,0,0], [0,1,1] ])\n",
    "clusters = clust.KMeans(n_clusters=2, random_state=42, n_init=\"auto\").fit(data)\n",
    "print(clusters.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9774b3-faf4-4495-bd81-0ec843c6eed5",
   "metadata": {},
   "source": [
    "# Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d666c01e-9790-438d-b4dd-92391b81e466",
   "metadata": {},
   "source": [
    "For $\\overrightarrow{u}$ and $\\overrightarrow{v}$, two vectors :\n",
    "$$ cos (\\overrightarrow{u}, \\overrightarrow{v}) = \\frac{\\langle \\overrightarrow{u} \\; , \\; \\overrightarrow{v} \\rangle}{ \\lVert \\overrightarrow{u} \\rVert .\\lVert \\overrightarrow{v} \\rVert }  $$\n",
    "This value is called the cosine similarity and evaluate the angle between two vectors (can be relevant in some contexts) \\\n",
    "Fun fact : in the probability world, $ \\mathbb{E}(X.Y) $ could be considered as a dot-product-like operation (the associated norm then would be $\\sqrt{\\mathbb{E}(X^2)}$), and the associated cosine similarity would be :\n",
    "$$ similarity(X,Y) = \\frac{ \\mathbb{E}(X.Y) }{\\sqrt{\\mathbb{E}(X^2)\\mathbb{E}(Y^2)}} $$\n",
    "And \n",
    "$$ similarity(X-\\mathbb{E}(X),Y-\\mathbb{E}(Y)) =  \\frac{ \\mathbb{E}((X-\\mathbb{E}(X)).(Y-\\mathbb{E}(Y))) }{\\sqrt{\\mathbb{E}((X-\\mathbb{E}(X))^2)\\mathbb{E}((Y-\\mathbb{E}(Y))^2)}} = \\frac{Cov(X,Y)}{\\sigma_X.\\sigma_Y} = Corr(X,Y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10968969-fd7e-4351-977d-926d23fc7775",
   "metadata": {},
   "source": [
    "# Chi2 Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff47fdd1-bd37-4869-9d8d-fe13532df667",
   "metadata": {},
   "source": [
    "The independance $ \\chi^2 $ test is a statistic used to test of two features of the same sample are independant \\\n",
    "For example let's take a sample of $n$ annotations $ A_1, \\dots, A_n $. We focus on two features : the fact that the annation was correct (noted $ x_k $ for the $k^{th}$ annotation), which can be YES or NO ; and the dataset from which the rated tweet comes from (noted $ y_k $ for the $k^{th}$ annotation), which can be ISARC or SIGN (in this example, there are only two possible values for each feature, but actually, there can be more). \\\n",
    "It is a hypothesis test, with $ \\mathcal{H_0} : \\text{The two features are independant} $ and $ \\mathcal{H_1} : \\text{The two features are not independant} $ \\\n",
    "Let $I = \\{ Y, N \\}$ be the set of all possible values taken by the first feature, and $J = \\{ ISARC, SIGN \\}$ be the set of all possible values taken by the second feature \\\n",
    "Let $N_{i,j}$ be the number of observed annotations for which the first feature is equal to $ i $ and the second feature is equal to $ j $ \\\n",
    "Let $N_{i,.}$ be the number of observed annotations for which the first feature is equal to $ i $ \\\n",
    "Let $N_{.,j}$ be the number of observed annotations for which the second feature is equal to $ j $ \\\n",
    "If the two features are independant (= under $\\mathcal{H}_0$), then :\n",
    "$$ \\mathbb{P}( x_k = i , y_k = j) =\\mathbb{P}( x_k = i)\\times \\mathbb{P}(y_k = j)  $$ \n",
    "Which should be observed in our sample with :\n",
    "$$ N_{i,j} \\approx \\frac{N_{i,.} \\times N_{.,j}}{n}$$\n",
    "We use the following test statistic :\n",
    "$$ T = \\sum_{i \\in I, j \\in J} \\frac{ \\left( N_{i,j} - \\frac{N_{i,.} \\times N_{.,j}}{n} \\right)^2 }{ \\frac{N_{i,.} \\times N_{.,j}}{n} } $$ \n",
    "(It actually looks like a distance between all the $ N_{i,j} $ and the $ \\frac{N_{i,.} \\times N_{.,j}}{n}$, weighted using $ \\frac{N_{i,.} \\times N_{.,j}}{n} $) \\\n",
    "Under $ \\mathcal{H}_0 $, we have $T \\sim \\chi^2( (|I| - 1)(|J| - 1) )$ and therefore, we reject $\\mathcal{H}_0$ with a Type I error of $\\alpha$ if :\n",
    "$$ T > \\chi_{(|I| - 1)(|J| - 1)}(1 - \\alpha) \\qquad (\\leftarrow \\text{\\tiny quantile of the chi2 distribution})$$ \n",
    "and the p-value is \n",
    "$$ \\hat{\\alpha} = 1 - F_{  \\chi^2( (|I| - 1)(|J| - 1) ) }( T ) \\qquad (\\leftarrow \\text{\\tiny cdf of the chi2 distribution}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1276e0-5a41-431d-9c3d-c69f411f85ae",
   "metadata": {},
   "source": [
    "For the example, let's imagine we have the following result :\n",
    "$$ \n",
    "\\begin{array}{|c|cc|}\n",
    "\\hline\n",
    " & CORRECT & INCORRECT \\\\\n",
    " \\hline\n",
    " SIGN & 5000 & 2500 \\\\\n",
    " ISARC & 3000 & 1580 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "(this table is called the contingency table of the observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26e83eaa-43fc-49eb-97cc-fce85098b932",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'statistic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21271/3373011291.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1580\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchi2_contingency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The test statistic : {res.statistic}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The p-value : {res.pvalue}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"That means we need to consider a risk of at least {round(res.pvalue * 100, 2)}% if we want to reject the null hypothesis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'statistic'"
     ]
    }
   ],
   "source": [
    "obs = np.array( [[5000, 2500],[3000, 1580]] ) \n",
    "res = stats.chi2_contingency(obs)\n",
    "print(f\"The test statistic : {res.statistic}\")\n",
    "print(f\"The p-value : {res.pvalue}\")\n",
    "print(f\"That means we need to consider a risk of at least {round(res.pvalue * 100, 2)}% if we want to reject the null hypothesis\")\n",
    "print(f\"Usually, a p-value greater than 5% (or even 1% sometimes) is considered as too much risk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
